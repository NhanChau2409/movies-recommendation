{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "import requests\n",
    "from dotenv import dotenv_values\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, LongType"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SECRET = dotenv_values('.env')\n",
    "\n",
    "spark = (SparkSession.builder.master(\"local\")\n",
    "         .config(\"spark.jars\", SECRET[\"PSQL_JAR\"]) # Path to PSQL jar in local\n",
    "         .appName('etl')\n",
    "         .getOrCreate())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_in_database(df, table_name, mode='append'):\n",
    "    \"\"\"Load pyspark dataframe into PSQL database\"\"\"\n",
    "\n",
    "    jdbc_url = f'jdbc:postgresql://localhost:{SECRET[\"PORT\"]}/{SECRET[\"DATABASE_NAME\"]}'\n",
    "\n",
    "    (df.write\n",
    "     .format(\"jdbc\")\n",
    "     .option(\"url\", jdbc_url)\n",
    "     .option(\"driver\", \"org.postgresql.Driver\")\n",
    "     .option(\"dbtable\", f'{table_name}')\n",
    "     .option(\"user\", SECRET[\"DATABASE_USERNAME\"])\n",
    "     .option(\"password\", SECRET[\"DATABASE_PASSWORD\"])\n",
    "     .mode(mode)\n",
    "     .save())\n",
    "\n",
    "    print(f'Successfully write {table_name} dataframe into database')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_from_blob(filename, custom_schema):\n",
    "    \"\"\"Read CSV file, from MovieLens dataset, with defined schema\"\"\"\n",
    "\n",
    "    current_directory = os.getcwd()\n",
    "    filepath = f'{current_directory}/blob/{filename}.csv'\n",
    "\n",
    "    df = (spark.read\n",
    "          .format(\"csv\")\n",
    "          .schema(custom_schema)\n",
    "          .option(\"header\", True)\n",
    "          .option(\"sep\", \",\")\n",
    "          .load(filepath))\n",
    "\n",
    "    print(f'Successfully read {filename} file from blob')\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rating_schema = StructType([\n",
    "    StructField(\"userId\", IntegerType(), False),\n",
    "    StructField(\"movieId\", IntegerType(), False),\n",
    "    StructField(\"rating\", FloatType(), False),\n",
    "    StructField(\"timestamp\", LongType(), False)\n",
    "])\n",
    "rating_df = read_from_blob('ratings', rating_schema)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "links_schema = StructType([\n",
    "    StructField(\"movieId\", IntegerType(), False),\n",
    "    StructField(\"imdbId\", IntegerType(), False),\n",
    "    StructField(\"tmdbId\", IntegerType(), False)\n",
    "])\n",
    "links_df = read_from_blob('links', links_schema)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specifically use inner join to remove incompatible data\n",
    "rating_df = rating_df.join(links_df, on='movieId', how='inner')\n",
    "\n",
    "rating_df = rating_df.drop('movieId', 'timestamp', 'imdbId').dropna()\n",
    "\n",
    "# Rename columns to database table columns name\n",
    "column_name_mapping = {'userId': 'user_id',\n",
    "                  'tmdbId': 'movie_id',\n",
    "                  'rating': 'rating'}\n",
    "rating_df = rating_df.select(*[F.col(old_name).alias(new_name) for old_name, new_name in column_name_mapping.items()])\n",
    "\n",
    "rating_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use TMDB data for more information about movie\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {SECRET['API_KEY']}\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "genres_url = \"https://api.themoviedb.org/3/genre/movie/list?language=en\"\n",
    "response = requests.get(genres_url, headers=headers)\n",
    "response.raise_for_status()\n",
    "\n",
    "genres_pdf = pd.DataFrame(json.loads(response.text)['genres'])\n",
    "genres_pdf = genres_pdf.rename(columns={'name': 'genre'})\n",
    "genres_table = spark.createDataFrame(genres_pdf)\n",
    "\n",
    "genres_table.printSchema()\n",
    "\n",
    "load_in_database(genres_table, 'genres')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split movie_ids into 100 chunks\n",
    "movie_np_array = rating_df.select('movie_id').distinct().toPandas().values.reshape(-1)\n",
    "movie_chunks_list = np.array_split(movie_np_array, 100)\n",
    "\n",
    "# Some movies in dataset are outdated in TMDB database\n",
    "fail_request_movie_id = []\n",
    "\n",
    "for chunk in movie_chunks_list:\n",
    "    success_request_chunk = []\n",
    "\n",
    "    for movie_id in chunk:\n",
    "        try:\n",
    "            url = f\"https://api.themoviedb.org/3/movie/{movie_id}?language=en-US\"\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            success_request_chunk.append(json.loads(response.text))\n",
    "\n",
    "        except requests.exceptions.HTTPError:\n",
    "            fail_request_movie_id.append(movie_id)\n",
    "\n",
    "    pdf = pd.DataFrame(success_request_chunk)\n",
    "\n",
    "    # Filter out useful data\n",
    "    movies_pdf = pdf.loc[:, ['id', 'title', 'release_date', 'runtime', 'overview', 'popularity',\n",
    "                             'vote_average', 'vote_count', 'poster_path', 'backdrop_path']]\n",
    "    # Cast types\n",
    "    movies_pdf['release_date'] = pd.to_datetime(movies_pdf['release_date'])\n",
    "    movie_table = spark.createDataFrame(movies_pdf)\n",
    "    load_in_database(movie_table, 'movies')\n",
    "\n",
    "\n",
    "    # Normalize JSON format of list of dictionary columns - genres column\n",
    "    movie_genres_pdf = pdf.loc[:, ['id', 'genres']]\n",
    "    movie_genres_json = json.loads(movie_genres_pdf.to_json(orient='records'))\n",
    "    movie_genres_pdf = pd.json_normalize(data=movie_genres_json, record_path='genres', record_prefix='genres_', meta=['id'])\n",
    "\n",
    "    movie_genres_pdf = movie_genres_pdf[['id', 'genres_id']].drop_duplicates()\n",
    "    movie_genres_pdf = movie_genres_pdf.rename(columns={'id': 'movie_id', 'genres_id': 'genre_id'})\n",
    "    movie_genres_table = spark.createDataFrame(movie_genres_pdf)\n",
    "    load_in_database(movie_genres_table, 'movie_genres')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fail_request_movie_id = [int(value) for value in fail_request_movie_id]\n",
    "\n",
    "# Filter out not existed movie\n",
    "rating_table = rating_df.filter(~rating_df.movie_id.isin(fail_request_movie_id))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user_table = rating_table.select(rating_table.user_id.alias('id')).distinct()\n",
    "\n",
    "# Generalize username, password for user in dataset\n",
    "user_table = user_table.withColumn('username',\n",
    "                                   F.concat(F.lit('username'), F.monotonically_increasing_id().cast('string')))\n",
    "user_table = user_table.withColumn('password',\n",
    "                                   F.concat(F.lit('password'), F.monotonically_increasing_id().cast('string')))\n",
    "load_in_database(user_table, 'users')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rating_table = rating_table.dropDuplicates(['user_id', 'movie_id'])\n",
    "\n",
    "load_in_database(rating_table, 'ratings')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
